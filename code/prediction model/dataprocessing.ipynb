{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read review_train.json and randomly select 1000000 reviews as training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('review_train.json','r') as f:\n",
    "    lines = f.readlines()\n",
    "raw = list(map(json.loads,lines))\n",
    "test = pd.DataFrame(raw).sample(n=1000000,random_state=123)\n",
    "# with open('review_test.json','r') as f:\n",
    "    #lines = f.readlines()\n",
    "#raw = list(map(json.loads,lines))\n",
    "#test = pd.DataFrame(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del lines\n",
    "del raw\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[['stars','text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stars = test.stars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transform emoticons to English words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def convert_emo(text):\n",
    "    happy = r\"(:-\\)|:\\)|:-\\]|:\\]|:3|:c\\)|:>|8\\)|8-\\)|:o\\)|=\\)|:\\}|:\\^\\)|:-\\)\\))\" \n",
    "    laugh = r\"(:‑D|:D|8‑D|8D|x‑D|xD|X‑D|XD|=‑D|=D|=‑3|=3|B\\^D)\" \n",
    "    sad = r\"(:‑\\(|:\\(|:‑c|:c|:‑<|:<|:‑\\[|:\\[|:\\{|;\\(|>:\\[)\" \n",
    "    angry = r\"(:-\\|\\||:@|>:\\()\" \n",
    "    cry = r\"(:'‑\\(|:'\\()\" \n",
    "    horror = r\"(D:<|D:|D8|D;|D=|DX|D‑':|v\\.v)\" \n",
    "    surprise = r\"(>:O|:‑O|:O|:‑o|:o|8‑0|O_O|o‑o|O_o|o_O|o_o|O-O)\" \n",
    "    kiss = r\"(:\\*|:-\\*|:\\^\\*|\\( '\\}\\{'\\))\" \n",
    "    wink = r\"(;‑\\)|;\\)|\\*-\\)|\\*\\)|;‑\\]|;\\]|;D|;\\^\\)|:‑,)\" \n",
    "    skeptical = r\"(>:\\\\\\\\|>:/|:‑/|:‑\\.|:/|:\\\\\\\\|=/|=\\\\\\\\|:L|=L|:S|>\\.<)\" \n",
    "    text = re.sub(happy,' happy ',text)\n",
    "    text = re.sub(laugh,' laugh ',text)\n",
    "    text = re.sub(sad,' sad ',text)\n",
    "    text = re.sub(angry,' angry ',text)\n",
    "    text = re.sub(cry,' cry ',text)\n",
    "    text = re.sub(horror,' horror ',text)\n",
    "    text = re.sub(surprise,' surprise ',text)\n",
    "    text = re.sub(kiss,' kiss ',text)\n",
    "    text = re.sub(wink,' wink ',text)\n",
    "    text = re.sub(skeptical,' skeptical ',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.text = test.text.apply(convert_emo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove \\n _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_gangn(text):\n",
    "    text = re.sub('\\n',' ',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.text = test.text.apply(remove_gangn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_underline(text):\n",
    "    text = re.sub('_',' ',text.lower())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.text = test.text.apply(rm_underline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.index = list(range(len(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the review with only punctuation to 'speechless'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test)):\n",
    "    if not bool(re.search('[A-Za-z]',test.text.loc[i])):\n",
    "        test.text.loc[i] = 'speechless'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "expand abbreviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_abb(text):\n",
    "    text = re.sub('n\\'t',' not',text)\n",
    "    text = re.sub('\\'m',' am',text)\n",
    "    text = re.sub('\\'s', ' is',text)\n",
    "    text = re.sub('\\'ve',' have',text)\n",
    "    text = re.sub('\\'d',' would',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.text = test.text.apply(convert_abb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "handle negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def add_negation(text):\n",
    "    negation = ['not', 'never', 'no']\n",
    "    words = text.split()[:]\n",
    "    flag = 0\n",
    "    for index, word in enumerate(words):\n",
    "        if(flag != 0):\n",
    "            words[index] = 'not_' + words[index]\n",
    "        if(re.match('.*[;,.?!]$', word)):\n",
    "            flag = 0\n",
    "        if(word in negation):\n",
    "            flag = 1\n",
    "    text = \" \".join(words)\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.text = test.text.apply(add_negation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove all punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(text):\n",
    "    text = re.sub('[/*-?!:;,\\.\\'\\\"\\)\\(]', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.text = test.text.apply(remove_punc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer_all(text):\n",
    "    new_text = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for word, tag in pos_tag(word_tokenize(text)):\n",
    "        if tag.startswith('NN'):\n",
    "            new_text.append(lemmatizer.lemmatize(word, pos='n'))\n",
    "        elif tag.startswith('VB'):\n",
    "            new_text.append(lemmatizer.lemmatize(word, pos='v'))\n",
    "        elif tag.startswith('JJ'):\n",
    "            new_text.append(lemmatizer.lemmatize(word, pos='a'))\n",
    "        elif tag.startswith('R'):\n",
    "            new_text.append(lemmatizer.lemmatize(word, pos='r'))\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    return ' '.join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.text = test.text.apply(lemmatizer_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove no importance words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_importance_pos = ['IN','MD','PRP','PRP$','TO','WDT','WP','WP$','WRB']\n",
    "no_importance_word = ['i','he','she','they','the','a','an','this','that','those','be','not_the','there','not_to','not_a','not_me','not_my',\n",
    "                      'not_in','not_of','not_or','not_is','not_was','not_for','not_not_be','do_not','then','and','not','always','go','have',\n",
    "                      'do','get','so','one','just','all','not_and','so', 'say','here','ever','not_get','not_i','not_that','not_this',\n",
    "                      'not_not','guy','some','call','guy','re','one','not_not_even','then','and','not','always','go','have','do','get',\n",
    "                      'so','one','just','all','not_and', 'say','here','ever','not_get','not_i','not_that','not_this','not_not','guy','some',\n",
    "                      'guy','re','one', 'not_have','not_be','not_','not_it','not_be','not_you','not_on','even','not_you',\n",
    "                      'not_on','up','ll','now','too','need', 'really','now','two']\n",
    "only_text = []\n",
    "index = []\n",
    "for i in range(len(test)):\n",
    "    try:\n",
    "        only_text.append(' '.join(test.text.loc[i].split(' ')))\n",
    "    except:\n",
    "        index.append(i)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = []\n",
    "index2 = []\n",
    "for i in range(len(only_text)):\n",
    "    try:\n",
    "        retain = []\n",
    "        POS = pos_tag(only_text[i].split(' '))\n",
    "        for word in POS:\n",
    "            if word[1] not in no_importance_pos and word[0] not in no_importance_word and len(word[0])!= 1:\n",
    "                retain.append(word[0])\n",
    "        new_text.append(' '.join(retain))\n",
    "    except:\n",
    "        index2.append(i)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_word = [sentence.split(' ') for sentence in new_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ph = Phraser(Phrases(only_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_text = []\n",
    "for i in range(len(only_word)):\n",
    "    final_text.append(' '.join(ph[only_word[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [stars,final_text]\n",
    "data = pd.DataFrame(data)\n",
    "data = data.T\n",
    "data.rename(columns = {0:'stars',1:'text'},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('train_process.csv',index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
